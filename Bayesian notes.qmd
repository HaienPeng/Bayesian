---
title: "Bayesian notes"
format:
  pdf:
    pdf-engine: xelatex
    documentclass: ctexart
editor: visual
---

# 1 在做什么

在估计参数, $p(\theta|y)=\frac{p(y|\theta)p(\theta)}{p(y)} = \frac{p(y|\theta)p(\theta)}{\int p(y|\theta)p(\theta) \,d\theta}$, 我们想知道参数$\theta$的后验分布是什么, 但是所知道的东西只有prior $p(\theta)$和likelihood$p(y|\theta)$. 而现实中往往面临的情况是分母这个积分根本没有办法积出来.

# 2 怎么做

一共有三种方法估计参数:\
(1) Analytic: 直接能求出来$p(\theta|y)$的具体表达式\
(2) Sampling: 用随机样本逼近后验分布. 例如:Gibbs, Metropolis–Hastings, HMC\
(3) Approximation:用优化或展开近似分布

## 2.1 Analytic

什么时候能直接写出后验？当先验与似然属于“共轭”组合时，后验与先验同族，只是参数更新了一下。这样无需数值方法，可直接写出后验分布。 常见的共轭对有:\
(1) 正态均值\
prior: $\mu \sim N(m,s^2)$\
likelihood: $y_i|\mu\sim N(\mu,\sigma^2)$ ($\sigma^2$已知)\
posterior: $\mu|y \sim N(\hat{m},\hat{s}^2)$\
(2) 正态线性回归\
Conjugate priors: $\beta|\sigma^2 \sim N(B,\sigma^2\sum), \sigma^2 \sim Inv-Gamma(a,b)$\
Posterior: $\beta|\sigma^2,y$仍为正态,$\sigma^2|y$为逆伽马\

## 2.2 Sampling

### Direct Sampling

首先要回答第一个问题,什么是采样,如果我说我从正态分布$N(0,1)$里取1个随机数是什么意思.先说过程,我先从$Uniform(0,1)$中取出一个数$y$ (可以闭眼从1cm的直尺上随机指出一个数),接着使用$\Phi(z)=y$计算出$z$的值,那么这就是我们从$N(0,1)$中取出的一个随机数,为什么?接下来给出证明:\
(1) 目的是要证明这样计算出的$Z$应该服从正态分布: $p(Z\leq z) = \Phi(z)$\
(2) $Z$ 是由 $\Phi(Z)=Y$ 也就是 $\Phi^{-1}(Y)$ 计算出来的,因此$p(Z\leq z) =  p(\Phi^{-1}(Y)\leq z)=p(Y\leq \Phi(z)) =\Phi(z)$ (中间等号根据分布函数的单调性和反函数的性质)\

```{r}
U = runif(1000,0,1) # 生成1000个服从U(0,1)的数
hist(U)
Z = qnorm(U) # 做正态分布的逆变换
hist(Z)
```

这个方法叫做直接采样(Direct Sampling),但缺点显而易见,我们必须能知道怎么计算后验分布的反函数,但很多分布太过于复杂根本没办法求解反函数,就需要一些办法来采样.\

### Rejection Sampling

拒绝采样的原理并不复杂,这个方法依赖于我们提出一个proposal density, 要求这个提议分布必须得完全包裹住我们想要采样的目标分布,也就是条件$cq(x) \geq p(x)$,这个常数项 c 只是为了保证proposal density一定能包裹住目标分布,接着想象在$cq(x)$曲线上随机撒点,我们只留下那些在目标分布$p(x)$曲线下的点,对于每个从$q(x)$里取样出来的点,是否接受它们取决于它们自己的权重,这个权重用$U \leq\frac{p(X)}{cq(X)}$来表示,为什么?如果定义事件A为接受这些点,那么$P(A)=P(U \leq\frac{p(X)}{cq(X)})$,只有当满足$U \sim U(0,1)$的时候$P(A)=P(U \leq\frac{p(X)}{cq(X)})=\frac{p(X)}{cq(X)}$,意味着此时接受X的概率正好等于这个点自己在目标分布$q(x)$曲线下方的概率,那当把所有这些接受的X都收集起来,就能得到目标分布的分布曲线.\
以下是个简单的例子如何来运行拒绝采样.我们的目标分布是$f(x)=2x$,提议分布是$U(0,1)$,这样令$c=2$正好使得提议分布在目标分布之上,步骤一共五步:\
(1) 选择$c$\
(2) 从提议分布里采样一个数$x_i$\
(3) 从$U(0,1)$里采样一个数$u_i$\
(4) 比较$u_i \leq \frac{p(x_i)}{cq(x_i)}$是否成立,若成立则留下$x_i$\
(5) 重复以上步骤多次直到生成完整的样本\

```{r}
set.seed(123)

n <- 10000                # 想要的样本数
accepted <- c()           # 存放被接受的样本
cst <- 2                  # 因为 p(x)/q(x)=2x，最大值是 2，所以 c = 2 就够

while (length(accepted) < n) {
  x <- runif(1)           # 从 U(0,1) 提样
  u <- runif(1)           # 从 U(0,1) 提样作为接受概率
  if (u < (2 * x) / cst) {  # 接受条件
    accepted <- c(accepted, x)
  }
}

# 画图对比目标密度
hist(accepted, breaks = 50, freq = FALSE, col = "grey90",
     main = "Rejection Sampling: target p(x)=2x, proposal U(0,1)",
     xlab = "x")
curve(2*x, from = 0, to = 1, add = TRUE, lwd = 2, col = "red")
legend("topleft", legend = c("Target density f(x)=2x"), col = "red", lwd = 2)


```

拒绝性采样的缺陷来自几点：\
（1）效率低,一旦proposal density和taget density差距很大,在判别此处时$U \leq\frac{p(X)}{cq(X)}$就会丢弃非常多的样本\
（2）需要知道上界常数 $c$ ,使用该方法必须满足 $cq(x) \geq p(x)$ , 但在分布非常复杂时无法做到这一点

### Importance Sampling

重要性采样的原理也并不复杂,核心思想是：用一个容易采样的分布来近似原本难以采样的目标分布，并通过加权修正来保证估计无偏。\

比如说要计算的期望是$E[f(X)]=\int f(x)p(x)dx$,很多情况下 $p(x)$ 太过于复杂这个积分没有办法计算,我们转而采用Monte Carlo的办法模拟近似估计:$\hat{E[f(X)]}=\frac{1}{n}\sum f(x_i)$,但问题在于 $x_i$ 都需要从分布 $p(x)$ 中直接采样得到,但很难做到这一点,所以才进行了数学变换$E_p[f(X)]=\int f(x)p(x)dx=\int \frac{f(x)p(x)}{q(x)}q(x)dx=E_q[\frac{f(X)p(X)}{q(X)}]$,通过这样的方式,此时如果我们再用Monte Carlo来近似的话,样本只需要从我们设计的proposal density$p(x)$里采样即可,此时再用Monte Carlo计算估计量表示为:$\hat{E[f(X)]}=\frac{1}{n}\sum \frac{f(x_i)p(x_i)}{q(x_i)}$,值得注意的是这个估计量虽然是无偏的但是方差却很大,因此我们实际使用的估计量是:$\hat{E[f(X)]}=\sum f(x_i)\widetilde{w_i}$,其中$\widetilde{w_i}=\frac{w_i}{\sum w_j}, w_i=\frac{p(x_i)}{q(x_i)}$,这个估计量的推导原理也很简单:$E_p[f(X)]=\frac{\int f(x)p(x)dx}{\int p(x)dx}=\frac{\int \frac{f(x)p(x)}{q(x)}q(x)dx}{\int \frac{p(x)}{q(x)}q(x)}=\frac{E_q[\frac{f(X)p(X)}{q(X)}]}{E_q[\frac{p(X)}{q(X)}]}$, 分母上的 $\int p(x)dx=1$ 因为这是pdf的积分,此外这里也能看得出来一个明显的区别就是如果我们只知道 $p(x)\propto g(x)$ 的话,那在前一个估计量处进行计算 $\hat{E[f(X)]}=\frac{1}{n}\sum \frac{f(x_i)p(x_i)}{q(x_i)}$ 就是不正确的,因为使用的 $p(x)$ (其实是 $g(x)$ )是一个未归一化的函数,所谓的未归一化指的是$\int p(x)dx=1$ 但是如果 $p(x)\propto g(x)$ 那么 $1=\int p(x)dx=\int \theta g(x)dx, \int g(x)dx=\frac{1}{\theta}$\

以下展示了个具体的例子,假如说我的taget density是一个Gaussian Mixture Distribution,这里的分布函数应该满足 $p(x)=0.7N(-1,1)+0.3N(6,1)$ ,值得注意的是这个分布和$Y=0.7X_1+0.3X_2, where X_1 \sim N(-1,1), X_2 \sim N(6,1)$ 完全不同,在后一种情形下 $Y$ 仍旧是一个正态分布, 而在前一种情形下的pdf则是由两个正态分布的pdf直接相加形成的.代码中使用了 $q(x) \sim N(0,2)$ 作为proposal density,并且当从提议分布中抽样后,根据weight再次重新从已经抽出的样本中抽样画出了图.

```{r importance, fig.show='hold', echo=TRUE}

set.seed(1234)

# --- 目标分布：混合高斯 ---
x <- c(rnorm(7000,-1,1), rnorm(3000,6,1))

# --- 提议分布：简单高斯 ---
m <- 100000
g <- rnorm(m, 0, 2)
g.sorted <- sort(g)

# --- 计算权重 w = p/q ---
weights <- (0.7*dnorm(g.sorted, -1, 1) + 0.3*dnorm(g.sorted, 6, 1)) / 
            dnorm(g.sorted, 0, 2)

# --- 归一化 + 重采样 ---
w_norm <- weights / sum(weights)
resample_idx <- sample(1:length(g.sorted), size=1000, replace=TRUE, prob=w_norm)
x_resampled <- g.sorted[resample_idx]

# --- 可视化 ---
plot(density(x), col="black", lwd=2, main="Importance Sampling + Resampling", 
     xlab="x", xlim=c(-5,10),ylim=c(0, 0.6))
lines(density(g.sorted), col="gray", lwd=2, lty=2)
lines(density(x_resampled), col="blue", lwd=2)
legend("topright", legend=c("Target p(x)", "Proposal q(x)", "Resampled Samples"),
       col=c("black", "gray", "blue"), lty=c(1,2,1), lwd=2)



```

### Markov Chain Monte Carlo (MCMC)

MCMC的目标也是为了生成taget density的样本,Markov Chain的意思是一个随机过程，其中下一个状态只依赖于当前状态，而与过去的历史无关,这就是所谓的 “无记忆性（Markov property）”,在贝叶斯推断中，我们想构造这样一个“链”：它一步一步“走”，每个新状态（参数值）只依赖于上一个状态，当走得够久时访问每个状态的频率刚好符合后验分布$p(\theta|D)$. 所有接下来的所有算法都是围绕怎么构造这样一条链出发的\

#### Metropolis-Hastings

先讲这个算法生成整条链的步骤:

(1) 从初始点 $\theta^{(0)}$ 开始
(2) 随机提议一个新的点 $\theta'$ ,这个点由提议分布计算得来: $\theta' \sim q(\theta'|\theta^{(0)})$
(3) 接着根据Acceptance ratio决定是否接受这个点: $$
    \alpha(\theta, \theta') = \min\left(1, \frac{p(\theta')\, q(\theta^{(t)} \mid \theta')}{p(\theta^{(t)})\, q(\theta' \mid \theta^{(t)})} \right)
    $$ 如果 $\alpha=1$ 一定接受,如果 $\alpha<1$,则再成一个随机数 $u \sim U(0,1)$, 如果 $u<\alpha$, 就接受新的点,否则拒绝\
    以下给出一个代码示例看看究竟是怎么做的,假设我们的target density就是 $p(x) \sim N(0,1)$, proposal density是 $q(\theta'|\theta^{(t)})=N(\theta^{(t)},1)$, 随机生成规则可以写成 $\theta'=\theta^{(t)}+\epsilon , \epsilon \sim N(0,1)$,此时可以看到由于提议分布完全对称所以Acceptance ratio可以简化成$\alpha(\theta, \theta') = \min\left(1, \frac{p(\theta') }{p(\theta^{(t)})} \right)$, 因为$q(\theta' \mid \theta^{(t)})= \frac{1}{\sqrt{2\pi}}e^{-\frac{(\theta'-\theta^{(t)})^2}{2}}$, 就相当于我设定了 $\theta^{(t)}$ 具体是多少,那我现在这个分布函数里就没有不知道的参数了,我可以从这个分布函数里抽样出一个值作为 $\theta'$.

```{r}
# Metropolis–Hastings for target N(0,1) with symmetric Gaussian proposal in R

mh_normal <- function(n_samples = 10000,
                      proposal_sd = 1.0,  # 提议步长的标准差
                      theta0 = 0.0,
                      burn_in = 1000,
                      thin = 1,
                      seed = 123) {
  set.seed(seed)
  n_total <- n_samples + burn_in
  samples <- numeric(n_total)
  theta <- theta0
  acc <- 0L
  
  # 目标对数密度：标准正态 (忽略常数也可以，这里用dnorm的log形式最稳)
  logp <- function(x) dnorm(x, mean = 0, sd = 1, log = TRUE)
  
  for (t in seq_len(n_total)) {
    # 对称高斯提议：theta' = theta + eps, eps ~ N(0, proposal_sd^2)
    # 也可以写成theta_prop <- rnorm(1, mean = theta, sd = proposal_sd), 代表直接从这个条件分布中抽样
    theta_prop <- theta + rnorm(1, mean = 0, sd = proposal_sd)

    
    # 接受率（对称提议：q 互相抵消），用log-接受率更数值稳定
    # log接受率数值更稳定的意思是算出来的数太大或者太小计算机误以为是0,先取对数之后再进行指数运算方便
    log_alpha <- logp(theta_prop) - logp(theta)
    alpha <- exp(min(0, log_alpha))  # 等价于 min(1, exp(log_alpha))
    
    if (runif(1) < alpha) {
      theta <- theta_prop
      acc <- acc + 1L
    }
    samples[t] <- theta
  }
  
  chain <- samples[(burn_in + 1):n_total]
  if (thin > 1) chain <- chain[seq(1, length(chain), by = thin)]
  acc_rate <- acc / n_total
  
  list(chain = chain, acceptance = acc_rate)
}

# ---- 运行示例 ----
res <- mh_normal(
  n_samples = 10000,
  proposal_sd = 1.0,  # 调大步长一般降接受率；调小步长提升接受率但移动慢
  theta0 = 0.0,
  burn_in = 1000,
  thin = 1,
  seed = 2025
)

cat(sprintf("接受率: %.3f\n", res$acceptance))
cat(sprintf("样本均值: %.3f, 样本方差: %.3f\n",
            mean(res$chain), var(res$chain)))

# 直方图对比真密度
hist(res$chain, breaks = 60, freq = FALSE, col = "grey80",
     main = "MH on N(0,1) with symmetric Gaussian proposal",
     xlab = "theta")
curve(dnorm(x, 0, 1), add = TRUE, lwd = 2)


```
